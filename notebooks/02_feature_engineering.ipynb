{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcc1874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING SUMMARY ===\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== FEATURE ENGINEERING SUMMARY ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEngineered features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_scaled.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature increase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_scaled.shape[\u001b[32m1\u001b[39m]\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m new features\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=== FEATURE ENGINEERING SUMMARY ===\\n\")\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"Engineered features: {X_scaled.shape[1]}\")\n",
    "print(f\"Feature increase: {X_scaled.shape[1] - df.shape[1]} new features\\n\")\n",
    "\n",
    "print(\"Feature Categories:\")\n",
    "print(f\"  - Spatial features: 11 (distance, coordinates, bins, boundaries)\")\n",
    "print(f\"  - Interaction features: 9 (climate-vegetation, urban, biodiversity)\")\n",
    "print(f\"  - Ecological domain features: 5 (biodiversity, threat, conservation indices)\")\n",
    "print(f\"  - Statistical features: 13+ (polynomials, ratios, percentiles)\")\n",
    "print(f\"  - Original features: {len(df.columns)}\\n\")\n",
    "\n",
    "print(\"Feature Engineering Techniques Applied:\")\n",
    "print(\"  ✓ Spatial transformations (distance, bins, boundary distance)\")\n",
    "print(\"  ✓ Interaction terms (climate×vegetation, urban×climate)\")\n",
    "print(\"  ✓ Domain-specific indices (biodiversity, threat, conservation)\")\n",
    "print(\"  ✓ Polynomial features (2nd order)\")\n",
    "print(\"  ✓ Ratio and composite features\")\n",
    "print(\"  ✓ Standardization (zero mean, unit variance)\")\n",
    "print(\"  ✓ Feature importance ranking (correlation, MI, RF)\")\n",
    "print(\"  ✓ Dimensionality reduction (PCA)\\n\")\n",
    "\n",
    "print(f\"Next steps for modeling:\")\n",
    "print(f\"  1. Use all {X_scaled.shape[1]} features for baseline models\")\n",
    "print(f\"  2. Try top {n_components_95} PCA components for comparison\")\n",
    "print(f\"  3. Use top 15-20 features (by RF importance) for interpretability\")\n",
    "print(f\"  4. Ensemble different feature sets for robustness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041727e",
   "metadata": {},
   "source": [
    "## 9. Final Feature Set Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a438c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to capture variance\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate cumulative variance explained\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot variance explained\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance Explained Ratio')\n",
    "axes[0].set_title('Variance Explained by Each Component')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, 'bo-')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Explained')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "\n",
    "print(f\"Components for 90% variance: {n_components_90}\")\n",
    "print(f\"Components for 95% variance: {n_components_95}\")\n",
    "print(f\"Original features: {X_scaled.shape[1]}\")\n",
    "print(f\"Dimensionality reduction: {n_components_95}/{X_scaled.shape[1]} = {n_components_95/X_scaled.shape[1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e445d",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d719ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Correlation\n",
    "top_corr = correlations.head(10)\n",
    "axes[0].barh(top_corr['feature'], top_corr['abs_correlation'])\n",
    "axes[0].set_xlabel('Absolute Correlation')\n",
    "axes[0].set_title('Top 10 Features by Correlation')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Mutual Information\n",
    "top_mi = mi_scores.head(10)\n",
    "axes[1].barh(top_mi['feature'], top_mi['mutual_info'])\n",
    "axes[1].set_xlabel('Mutual Information')\n",
    "axes[1].set_title('Top 10 Features by Mutual Information')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Random Forest\n",
    "top_rf = rf_importance.head(10)\n",
    "axes[2].barh(top_rf['feature'], top_rf['importance'])\n",
    "axes[2].set_xlabel('Importance Score')\n",
    "axes[2].set_title('Top 10 Features by Random Forest')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21adfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance using multiple methods\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 1. Correlation with target\n",
    "correlations = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'correlation': [X_scaled[col].corr(y) for col in X_scaled.columns]\n",
    "})\n",
    "correlations['abs_correlation'] = correlations['correlation'].abs()\n",
    "correlations = correlations.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "# 2. Mutual information\n",
    "selector = SelectKBest(mutual_info_regression, k=min(20, X_scaled.shape[1]))\n",
    "selector.fit(X_scaled, y)\n",
    "mi_scores = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'mutual_info': selector.scores_\n",
    "}).sort_values('mutual_info', ascending=False)\n",
    "\n",
    "# 3. Random Forest feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_scaled, y)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top features\n",
    "print(\"=== TOP 15 FEATURES BY CORRELATION ===\")\n",
    "print(correlations.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TOP 15 FEATURES BY MUTUAL INFORMATION ===\")\n",
    "print(mi_scores.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== TOP 15 FEATURES BY RANDOM FOREST ===\")\n",
    "print(rf_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2f512",
   "metadata": {},
   "source": [
    "## 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e94b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features, categorical, and target\n",
    "feature_cols = [col for col in df_feat.columns if col not in ['risk_score', 'risk_category']]\n",
    "X = df_feat[feature_cols].copy()\n",
    "y = df_feat['risk_score'].copy()\n",
    "\n",
    "# Handle any NaN or infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Display scaling statistics\n",
    "print(\"Feature Scaling Summary:\")\n",
    "print(f\"  Original feature range: [{X.values.min():.2f}, {X.values.max():.2f}]\")\n",
    "print(f\"  Scaled feature range: [{X_scaled.values.min():.2f}, {X_scaled.values.max():.2f}]\")\n",
    "print(f\"  Features mean: {X_scaled.values.mean():.6f}\")\n",
    "print(f\"  Features std: {X_scaled.values.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8ba1a",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features for key variables (2nd order)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "climate_cols = ['temperature', 'precipitation', 'humidity']\n",
    "poly_features = poly.fit_transform(df_feat[climate_cols])\n",
    "poly_feature_names = poly.get_feature_names_out(climate_cols)\n",
    "\n",
    "for name, feature in zip(poly_feature_names[len(climate_cols):], poly_features[:, len(climate_cols):]):\n",
    "    if name not in df_feat.columns:\n",
    "        df_feat[name] = feature\n",
    "\n",
    "# Ratio features\n",
    "df_feat['endemic_to_total_species'] = df_feat['endemic_species'] / (df_feat['species_count'] + 1)\n",
    "df_feat['threatened_to_total_species'] = df_feat['threatened_species'] / (df_feat['species_count'] + 1)\n",
    "df_feat['temp_to_precip_ratio'] = df_feat['temperature'] / (df_feat['precipitation'] + 1)\n",
    "df_feat['humidity_to_precip_ratio'] = df_feat['humidity'] / (df_feat['precipitation'] + 1)\n",
    "\n",
    "# Normalization-based features\n",
    "df_feat['temp_percentile'] = df_feat['temperature'].rank(pct=True)\n",
    "df_feat['precip_percentile'] = df_feat['precipitation'].rank(pct=True)\n",
    "df_feat['elevation_percentile'] = df_feat['elevation'].rank(pct=True)\n",
    "\n",
    "print(\"✓ Statistical features created\")\n",
    "print(f\"  New features: polynomial terms, ratios, percentiles\")\n",
    "print(f\"\\nTotal features after engineering: {df_feat.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea33ec",
   "metadata": {},
   "source": [
    "## 5. Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad905e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biodiversity index\n",
    "df_feat['biodiversity_index'] = (\n",
    "    df_feat['species_count'] * 0.5 + \n",
    "    df_feat['endemic_species'] * 0.3 + \n",
    "    df_feat['threatened_species'] * 0.2\n",
    ")\n",
    "\n",
    "# Threat index (higher = more threatened)\n",
    "df_feat['threat_index'] = (\n",
    "    (1 - df_feat['forest_cover']) * 0.3 +\n",
    "    df_feat['urban_area'] * 0.25 +\n",
    "    df_feat['population_density'] / df_feat['population_density'].max() * 0.25 +\n",
    "    (df_feat['threatened_species'] / (df_feat['species_count'] + 1)) * 0.2\n",
    ")\n",
    "\n",
    "# Environmental stress index\n",
    "df_feat['temperature_stress'] = np.abs(df_feat['temperature'] - 25) / 10\n",
    "df_feat['precipitation_anomaly'] = np.abs(df_feat['precipitation'] - df_feat['precipitation'].mean()) / df_feat['precipitation'].std()\n",
    "\n",
    "# Conservation priority (based on biodiversity and threat)\n",
    "df_feat['conservation_priority'] = df_feat['biodiversity_index'] * (1 - df_feat['threat_index'])\n",
    "\n",
    "# Habitat quality indicator\n",
    "df_feat['habitat_quality'] = (\n",
    "    df_feat['forest_cover'] * 0.5 +\n",
    "    (1 - df_feat['urban_area']) * 0.3 +\n",
    "    (df_feat['water_bodies'] / 0.3) * 0.2  # normalize by typical max\n",
    ")\n",
    "df_feat['habitat_quality'] = np.clip(df_feat['habitat_quality'], 0, 1)\n",
    "\n",
    "print(\"✓ Ecological domain features created\")\n",
    "print(f\"  New features: biodiversity_index, threat_index, temperature_stress, conservation_priority, habitat_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b7b2d",
   "metadata": {},
   "source": [
    "## 4. Ecological Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate-vegetation interactions\n",
    "df_feat['temp_forest_interaction'] = df_feat['temperature'] * df_feat['forest_cover']\n",
    "df_feat['precip_forest_interaction'] = df_feat['precipitation'] * df_feat['forest_cover']\n",
    "df_feat['humidity_forest_interaction'] = df_feat['humidity'] * df_feat['forest_cover']\n",
    "\n",
    "# Urban-climate interactions\n",
    "df_feat['urban_temp_interaction'] = df_feat['urban_area'] * df_feat['temperature']\n",
    "df_feat['urban_pop_interaction'] = df_feat['urban_area'] * df_feat['population_density']\n",
    "\n",
    "# Biodiversity-climate interactions\n",
    "df_feat['species_temp_interaction'] = df_feat['species_count'] * df_feat['temperature']\n",
    "df_feat['species_precip_interaction'] = df_feat['species_count'] * df_feat['precipitation']\n",
    "\n",
    "# Land use composition\n",
    "df_feat['non_forest_cover'] = 1 - df_feat['forest_cover']\n",
    "df_feat['total_developed'] = df_feat['urban_area'] + df_feat['agricultural_area']\n",
    "df_feat['natural_area'] = df_feat['forest_cover'] + df_feat['water_bodies']\n",
    "\n",
    "print(\"✓ Interaction features created\")\n",
    "print(f\"  New features: climate-vegetation, urban-climate, biodiversity-climate interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a355b8",
   "metadata": {},
   "source": [
    "## 3. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77234c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate Haversine distance in km\"\"\"\n",
    "    R = 6371  # Earth's radius in km\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Create spatial features\n",
    "df_feat = df.copy()\n",
    "\n",
    "# Distance from center point\n",
    "center_lat = df_feat['latitude'].mean()\n",
    "center_lon = df_feat['longitude'].mean()\n",
    "\n",
    "df_feat['distance_from_center'] = df_feat.apply(\n",
    "    lambda row: calculate_distance(row['latitude'], row['longitude'], center_lat, center_lon),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Coordinate transformations\n",
    "df_feat['lat_squared'] = df_feat['latitude'] ** 2\n",
    "df_feat['lon_squared'] = df_feat['longitude'] ** 2\n",
    "df_feat['lat_lon_product'] = df_feat['latitude'] * df_feat['longitude']\n",
    "\n",
    "# Spatial bins\n",
    "df_feat['lat_bin'] = pd.cut(df_feat['latitude'], bins=10, labels=False)\n",
    "df_feat['lon_bin'] = pd.cut(df_feat['longitude'], bins=10, labels=False)\n",
    "\n",
    "# Distance to boundaries\n",
    "lat_min, lat_max = df['latitude'].min(), df['latitude'].max()\n",
    "lon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n",
    "\n",
    "df_feat['distance_to_north'] = lat_max - df_feat['latitude']\n",
    "df_feat['distance_to_south'] = df_feat['latitude'] - lat_min\n",
    "df_feat['distance_to_east'] = lon_max - df_feat['longitude']\n",
    "df_feat['distance_to_west'] = df_feat['longitude'] - lon_min\n",
    "\n",
    "print(\"✓ Spatial features created\")\n",
    "print(f\"  New features: distance_from_center, lat_squared, lon_squared, lat_lon_product, lat_bin, lon_bin, distance_to_*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6937b4b",
   "metadata": {},
   "source": [
    "## 2. Spatial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee332c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the same dataset as in data exploration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "lat_range = (15.6, 22.0)\n",
    "lon_range = (72.6, 80.9)\n",
    "\n",
    "data = {\n",
    "    'latitude': np.random.uniform(lat_range[0], lat_range[1], n_samples),\n",
    "    'longitude': np.random.uniform(lon_range[0], lon_range[1], n_samples),\n",
    "    'temperature': np.random.normal(25, 5, n_samples),\n",
    "    'precipitation': np.random.exponential(2, n_samples),\n",
    "    'humidity': np.random.normal(60, 15, n_samples),\n",
    "    'wind_speed': np.random.exponential(3, n_samples),\n",
    "    'forest_cover': np.random.uniform(0, 1, n_samples),\n",
    "    'agricultural_area': np.random.uniform(0, 1, n_samples),\n",
    "    'urban_area': np.random.uniform(0, 1, n_samples),\n",
    "    'water_bodies': np.random.uniform(0, 0.3, n_samples),\n",
    "    'species_count': np.random.poisson(15, n_samples),\n",
    "    'endemic_species': np.random.poisson(2, n_samples),\n",
    "    'threatened_species': np.random.poisson(1, n_samples),\n",
    "    'elevation': np.random.normal(500, 300, n_samples),\n",
    "    'population_density': np.random.exponential(100, n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generate risk score\n",
    "risk_score = (\n",
    "    0.3 * (1 - df['forest_cover']) +\n",
    "    0.2 * df['urban_area'] +\n",
    "    0.15 * np.abs(df['temperature'] - 25) / 10 +\n",
    "    0.1 * (1 / (df['species_count'] + 1)) +\n",
    "    0.1 * df['population_density'] / 1000 +\n",
    "    0.15 * np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "df['risk_score'] = np.clip(risk_score, 0, 1)\n",
    "df['risk_category'] = pd.cut(df['risk_score'], bins=[0, 0.3, 0.6, 1.0], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Original features: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd684a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abb739",
   "metadata": {},
   "source": [
    "# EcoPredict Feature Engineering\n",
    "\n",
    "This notebook demonstrates feature engineering techniques to create meaningful predictive features from raw ecological data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
