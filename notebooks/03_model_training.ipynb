{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c027be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. Dataset Information:\")\n",
    "print(f\"   - Total samples: {len(df):,}\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "print(f\"   - Training set: {len(X_train):,} samples ({100*len(X_train)/len(df):.0f}%)\")\n",
    "print(f\"   - Test set: {len(X_test):,} samples ({100*len(X_test)/len(df):.0f}%)\")\n",
    "\n",
    "print(f\"\\n2. Models Trained: {len(trained_models)}\")\n",
    "for name in trained_models.keys():\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(f\"\\n3. Best Model: {best_model_name}\")\n",
    "print(f\"   - Test R² Score: {best_results['Test R²']:.4f}\")\n",
    "print(f\"   - Test RMSE: {best_results['Test RMSE']:.4f}\")\n",
    "print(f\"   - Test MAE: {best_results['Test MAE']:.4f}\")\n",
    "print(f\"   - Cross-Val R²: {cv_df[cv_df['Model'] == best_model_name]['CV Mean R²'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n4. Top 5 Performing Models (by Test R²):\")\n",
    "top_models = results_df.nlargest(5, 'Test R²')\n",
    "for i, (idx, row) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Model']:20s} - Test R²: {row['Test R²']:.4f}\")\n",
    "\n",
    "print(f\"\\n5. Model Improvements:\")\n",
    "print(f\"   - Best Test RMSE: {results_df['Test RMSE'].min():.4f}\")\n",
    "print(f\"   - Best Test MAE: {results_df['Test MAE'].min():.4f}\")\n",
    "print(f\"   - Baseline (Linear Regression) R²: {results_df[results_df['Model'] == 'Linear Regression']['Test R²'].values[0]:.4f}\")\n",
    "print(f\"   - Best improvement: {best_results['Test R²'] - results_df[results_df['Model'] == 'Linear Regression']['Test R²'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n6. Model Artifacts Saved:\")\n",
    "print(f\"   - Models: {len(list(model_dir.glob('*.pkl'))) - 1} (+ scaler)\")\n",
    "print(f\"   - Results summary: model_comparison_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27481a68",
   "metadata": {},
   "source": [
    "## 9. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836298c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    model_path = model_dir / f\"{model_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = model_dir / \"scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✓ Saved scaler to {scaler_path}\")\n",
    "\n",
    "# Save results summary\n",
    "results_df.to_csv(model_dir / 'model_comparison_results.csv', index=False)\n",
    "print(f\"✓ Saved results summary to {model_dir / 'model_comparison_results.csv'}\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {model_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dcbb0",
   "metadata": {},
   "source": [
    "## 8. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "print(\"Performing 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='r2', n_jobs=-1)\n",
    "    cv_results.append({\n",
    "        'Model': model_name,\n",
    "        'CV Mean R²': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std(),\n",
    "        'CV Scores': cv_scores\n",
    "    })\n",
    "    print(f\"{model_name:20s}: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Visualize CV results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cv_means = [r['CV Mean R²'] for r in cv_results]\n",
    "cv_stds = [r['CV Std'] for r in cv_results]\n",
    "model_names = [r['Model'] for r in cv_results]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "ax.bar(x, cv_means, yerr=cv_stds, capsize=5, alpha=0.8)\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('5-Fold Cross-Validation Results')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20328a",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac56507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[-15:]  # Top 15\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.barh(np.array(X.columns)[indices], importances[indices])\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        ax.set_title(f'{model_name} - Top 15 Features')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc1fdc",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (Tree-based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = results_df.loc[results_df['Test R²'].idxmax(), 'Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_results = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test R² Score:  {best_results['Test R²']:.4f}\")\n",
    "print(f\"Test RMSE:      {best_results['Test RMSE']:.4f}\")\n",
    "print(f\"Test MAE:       {best_results['Test MAE']:.4f}\")\n",
    "print(f\"Train R² Score: {best_results['Train R²']:.4f}\")\n",
    "print(f\"Overfitting Gap: {(best_results['Train R²'] - best_results['Test R²']):.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Get predictions from best model\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_test_pred_best\n",
    "\n",
    "# Create analysis plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test, y_test_pred_best, alpha=0.6)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual Risk Score')\n",
    "ax.set_ylabel('Predicted Risk Score')\n",
    "ax.set_title(f'{best_model_name}: Actual vs Predicted')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals plot\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_test_pred_best, residuals, alpha=0.6)\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Predicted Risk Score')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residual Plot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(residuals.mean(), color='r', linestyle='--', label=f'Mean: {residuals.mean():.4f}')\n",
    "ax.set_xlabel('Residual Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Residuals')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Q-Q plot\n",
    "from scipy import stats\n",
    "ax = axes[1, 1]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot (Normality Check)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46483984",
   "metadata": {},
   "source": [
    "## 5. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. R² Score Comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, results_df['Train R²'], width, label='Train', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['Test R²'], width, label='Test', alpha=0.8)\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('R² Score Comparison (Train vs Test)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x - width/2, results_df['Train RMSE'], width, label='Train', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['Test RMSE'], width, label='Test', alpha=0.8)\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('RMSE Comparison (Train vs Test)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. MAE Comparison\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x - width/2, results_df['Train MAE'], width, label='Train', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['Test MAE'], width, label='Test', alpha=0.8)\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('MAE Comparison (Train vs Test)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Test R² Ranking\n",
    "ax = axes[1, 1]\n",
    "sorted_results = results_df.sort_values('Test R²', ascending=True)\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(sorted_results)))\n",
    "ax.barh(sorted_results['Model'], sorted_results['Test R²'], color=colors)\n",
    "ax.set_xlabel('Test R² Score')\n",
    "ax.set_title('Model Performance Ranking (by Test R²)')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, v in enumerate(sorted_results['Test R²']):\n",
    "    ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675edf1a",
   "metadata": {},
   "source": [
    "## 4. Model Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf069cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=0.01, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "training_results = []\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    training_results.append({\n",
    "        'Model': name,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ (Train R²: {train_r2:.3f}, Test R²: {test_r2:.3f})\")\n",
    "\n",
    "results_df = pd.DataFrame(training_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc13f0",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [col for col in df.columns if col != 'risk_score']\n",
    "X = df[feature_cols].copy()\n",
    "y = df['risk_score'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for feature names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"Train-test split: {len(X_train)}/{len(X_test)} ({100*len(X_train)/len(X):.0f}% / {100*len(X_test)/len(X):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df8df6",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data (same as feature engineering)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "lat_range = (15.6, 22.0)\n",
    "lon_range = (72.6, 80.9)\n",
    "\n",
    "data = {\n",
    "    'latitude': np.random.uniform(lat_range[0], lat_range[1], n_samples),\n",
    "    'longitude': np.random.uniform(lon_range[0], lon_range[1], n_samples),\n",
    "    'temperature': np.random.normal(25, 5, n_samples),\n",
    "    'precipitation': np.random.exponential(2, n_samples),\n",
    "    'humidity': np.random.normal(60, 15, n_samples),\n",
    "    'wind_speed': np.random.exponential(3, n_samples),\n",
    "    'forest_cover': np.random.uniform(0, 1, n_samples),\n",
    "    'agricultural_area': np.random.uniform(0, 1, n_samples),\n",
    "    'urban_area': np.random.uniform(0, 1, n_samples),\n",
    "    'water_bodies': np.random.uniform(0, 0.3, n_samples),\n",
    "    'species_count': np.random.poisson(15, n_samples),\n",
    "    'endemic_species': np.random.poisson(2, n_samples),\n",
    "    'threatened_species': np.random.poisson(1, n_samples),\n",
    "    'elevation': np.random.normal(500, 300, n_samples),\n",
    "    'population_density': np.random.exponential(100, n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generate risk score\n",
    "risk_score = (\n",
    "    0.3 * (1 - df['forest_cover']) +\n",
    "    0.2 * df['urban_area'] +\n",
    "    0.15 * np.abs(df['temperature'] - 25) / 10 +\n",
    "    0.1 * (1 / (df['species_count'] + 1)) +\n",
    "    0.1 * df['population_density'] / 1000 +\n",
    "    0.15 * np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "df['risk_score'] = np.clip(risk_score, 0, 1)\n",
    "\n",
    "# Feature engineering (simplified from notebook 2)\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Spatial features\n",
    "center_lat = df['latitude'].mean()\n",
    "center_lon = df['longitude'].mean()\n",
    "df['distance_from_center'] = df.apply(\n",
    "    lambda row: haversine_distance(row['latitude'], row['longitude'], center_lat, center_lon), axis=1\n",
    ")\n",
    "\n",
    "# Interaction features\n",
    "df['biodiversity_index'] = df['species_count'] * 0.5 + df['endemic_species'] * 0.3 + df['threatened_species'] * 0.2\n",
    "df['threat_index'] = (\n",
    "    (1 - df['forest_cover']) * 0.3 + \n",
    "    df['urban_area'] * 0.25 + \n",
    "    df['population_density'] / df['population_density'].max() * 0.25\n",
    ")\n",
    "df['habitat_quality'] = np.clip(df['forest_cover'] * 0.5 + (1 - df['urban_area']) * 0.3, 0, 1)\n",
    "\n",
    "# Interaction terms\n",
    "df['temp_forest_interaction'] = df['temperature'] * df['forest_cover']\n",
    "df['urban_pop_interaction'] = df['urban_area'] * df['population_density']\n",
    "\n",
    "print(f\"Dataset prepared: {df.shape}\")\n",
    "print(f\"Target variable (risk_score) statistics:\")\n",
    "print(f\"  Mean: {df['risk_score'].mean():.3f}\")\n",
    "print(f\"  Std: {df['risk_score'].std():.3f}\")\n",
    "print(f\"  Min: {df['risk_score'].min():.3f}\")\n",
    "print(f\"  Max: {df['risk_score'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09bd88",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0de06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9921c65",
   "metadata": {},
   "source": [
    "# EcoPredict Model Training\n",
    "\n",
    "Train and compare multiple machine learning models for ecological risk prediction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
